# Guide d'Utilisation - Bot de Trading par Reinforcement Learning

## üìã Vue d'ensemble

Ce projet est un **bot de trading automatis√©** utilisant le **Reinforcement Learning (RL)** pour trader sur le march√© Forex, sp√©cifiquement la paire **EUR/USD**. Il utilise des algorithmes d'apprentissage par renforcement comme **PPO**, **DQN**, et **A2C** pour apprendre des strat√©gies de trading rentables.

## üöÄ Installation

### 1. Pr√©requis
Assurez-vous d'avoir Python install√© (version 3.7+ recommand√©e).

### 2. Installation des d√©pendances

```bash
# Installer les versions sp√©cifiques de setuptools et pip (n√©cessaire pour gym==0.21.0)
pip install setuptools==65.5.0 pip==21 
pip install wheel==0.38.0

# Installer les d√©pendances principales
pip install -r requirements.txt

# Installer les d√©pendances de rl-baselines3-zoo (si le sous-module est pr√©sent)
pip install -r rl-baselines3-zoo/requirements.txt
```

### 3. Structure du projet
```
RL-trading-main/
‚îú‚îÄ‚îÄ config.py              # Configuration des chemins (DATA_PATH, LOGS_PATH, etc.)
‚îú‚îÄ‚îÄ requirements.txt       # D√©pendances Python
‚îú‚îÄ‚îÄ hyperparams/          # Hyperparam√®tres pour les algorithmes RL
‚îÇ   ‚îú‚îÄ‚îÄ default/          # Hyperparam√®tres par d√©faut
‚îÇ   ‚îî‚îÄ‚îÄ tuned/            # Hyperparam√®tres optimis√©s
‚îú‚îÄ‚îÄ rl_trading/           # Code principal du bot
‚îÇ   ‚îú‚îÄ‚îÄ data/             # Gestion des donn√©es Forex
‚îÇ   ‚îú‚îÄ‚îÄ environments/     # Environnements de trading (Gym)
‚îÇ   ‚îî‚îÄ‚îÄ utils/            # Utilitaires
‚îú‚îÄ‚îÄ notebooks/            # Notebooks Jupyter pour exp√©rimenter
‚îî‚îÄ‚îÄ illustrations/        # Graphiques de r√©sultats
```

## üìä Comment utiliser le bot

### M√©thode 1 : Utilisation via les Notebooks Jupyter (Recommand√© pour d√©buter)

Les notebooks sont la meilleure fa√ßon de comprendre et d'utiliser le bot :

1. **Test de l'environnement** : `notebooks/forex_environment_test.ipynb`
   - Teste l'environnement de trading de base
   - Montre comment cr√©er un environnement simple

2. **Exp√©riences RL compl√®tes** : `notebooks/forex_full_eurusd_rl_experiments.ipynb`
   - Entra√Æne des mod√®les RL (PPO, DQN, A2C)
   - Effectue l'optimisation des hyperparam√®tres
   - √âvalue les performances

3. **Analyse des meilleurs mod√®les** : `notebooks/forex_full_eurusd_best_rl_models_analysis.ipynb`
   - Analyse les r√©sultats des mod√®les entra√Æn√©s
   - Visualise les performances

### M√©thode 2 : Utilisation programmatique (Python)

#### Exemple 1 : Cr√©er un environnement de trading simple

```python
import pandas as pd
from rl_trading.environments import (
    Actions,
    ForexEnvBasic,
    ForexMarketOrderStrategyAllIn,
    ForexRewardStrategyLogPortfolioReturn,
    ForexTradingCostsStrategySpread
)
from rl_trading.data.forex import (
    ForexDataSource,
    load_processed_forex_data,
)
from config import DATA_PATH

# Charger les donn√©es Forex
forex_data = load_processed_forex_data(
    DATA_PATH, 
    ForexDataSource.FOREXTESTER, 
    pairs=['EURUSD'], 
    version='Agg'
)

# Cr√©er l'environnement de trading
env = ForexEnvBasic(
    target_prices_df=forex_data['EURUSD'],
    features_df=forex_data['EURUSD'].drop('<DT>', axis=1),
    portfolio_value=1000,  # Capital initial
    allowed_actions={Actions.SELL, Actions.CLOSE, Actions.BUY},
    market_order_strategy=ForexMarketOrderStrategyAllIn(),
    reward_strategy=ForexRewardStrategyLogPortfolioReturn(),
    trading_costs_strategy=ForexTradingCostsStrategySpread(spread=0.0001),
    include_in_obs=['position']
)

# Tester l'environnement
obs = env.reset()
for _ in range(100):
    action = env.action_space.sample()  # Action al√©atoire (pour test)
    obs, reward, done, info = env.step(action)
    if done:
        break

# Visualiser les r√©sultats
env.render()
```

#### Exemple 2 : Entra√Æner un mod√®le PPO

```python
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from rl_trading.environments import (
    Actions,
    ForexEnvBasic,
    ForexMarketOrderStrategyAllIn,
    ForexRewardStrategyLogPortfolioReturn,
    ForexTradingCostsStrategySpread
)

# Cr√©er l'environnement (comme dans l'exemple 1)
def make_env():
    # ... code de cr√©ation d'environnement ...
    return env

# Cr√©er un environnement vectoris√©
vec_env = DummyVecEnv([make_env])

# Cr√©er et entra√Æner le mod√®le PPO
model = PPO('MlpPolicy', vec_env, verbose=1)
model.learn(total_timesteps=100000)

# Sauvegarder le mod√®le
model.save("ppo_forex_trading")

# Charger et utiliser le mod√®le
model = PPO.load("ppo_forex_trading")
obs = vec_env.reset()
for i in range(1000):
    action, _states = model.predict(obs, deterministic=True)
    obs, rewards, dones, info = vec_env.step(action)
    if dones[0]:
        obs = vec_env.reset()
```

### M√©thode 3 : Utilisation avec RL Baselines3 Zoo

Le projet utilise un fork personnalis√© de **RL Baselines3 Zoo** pour l'entra√Ænement. Si le sous-module est configur√© :

```bash
# Entra√Æner un mod√®le avec des hyperparam√®tres
python -m rl_zoo3.train \
    --algo ppo \
    --env ForexFullEURUSD-v1 \
    --hyperparams-file hyperparams/tuned/ForexFullEURUSD-v6/ppo.yml \
    --tensorboard-log logs/

# √âvaluer un mod√®le entra√Æn√©
python -m rl_zoo3.enjoy \
    --algo ppo \
    --env ForexFullEURUSD-v1 \
    --folder logs/
```

## üéØ Concepts cl√©s

### Actions disponibles
- **BUY** : Ouvrir une position longue (acheter)
- **SELL** : Ouvrir une position courte (vendre)
- **CLOSE** : Fermer la position actuelle

### Environnements
- **ForexEnvBasic** : Environnement de base avec 3 actions
- Diff√©rentes variantes selon les actions autoris√©es

### Strat√©gies de r√©compense
- **ForexRewardStrategyLogPortfolioReturn** : R√©compense bas√©e sur le log du retour du portefeuille
- **ForexRewardStrategyWeightedLogPortfolioReturns** : Version pond√©r√©e

### Co√ªts de trading
- **ForexTradingCostsStrategySpread** : Co√ªts bas√©s sur le spread bid-ask
- **ForexTradingCostsStrategyRelativeFee** : Co√ªts bas√©s sur un pourcentage

### Algorithmes RL support√©s
- **PPO** (Proximal Policy Optimization) - Recommand√©
- **DQN** (Deep Q-Network)
- **A2C** (Advantage Actor-Critic)

## üìà R√©sultats attendus

Selon le README, le mod√®le PPO optimis√© a obtenu :
- **112.53%** de retour cumulatif sur la p√©riode de validation
- **46.31%** de retour cumulatif sur la p√©riode d'√©valuation
- Ratio de Sharpe de **3.26** (validation) et **1.49** (√©valuation)

‚ö†Ô∏è **Note importante** : Les r√©sultats sont obtenus dans un environnement **sans commission**. L'ajout de commissions r√©duit significativement la rentabilit√©.

## üîß Configuration

### Modifier les chemins dans `config.py`
```python
DATA_PATH = "/chemin/vers/vos/donnees"
LOGS_PATH = "/chemin/vers/logs"
HYPERPARAMS_PATH = "/chemin/vers/hyperparams"
```

### Hyperparam√®tres
Les hyperparam√®tres sont stock√©s dans des fichiers YAML :
- `hyperparams/default/` : Valeurs par d√©faut
- `hyperparams/tuned/` : Valeurs optimis√©es

## üìù Workflow typique

1. **Pr√©parer les donn√©es** : Utiliser `notebooks/forex_data_collection.ipynb` et `forex_data_preproc_eda.ipynb`
2. **Cr√©er des features** : Utiliser `forex_data_feature_engineering_basic.ipynb` ou `forex_data_feature_engineering_ta.ipynb`
3. **Entra√Æner un mod√®le** : Utiliser `forex_full_eurusd_rl_experiments.ipynb`
4. **Analyser les r√©sultats** : Utiliser `forex_full_eurusd_best_rl_models_analysis.ipynb`

## ‚ö†Ô∏è Avertissements

1. **Donn√©es requises** : Le bot n√©cessite des donn√©es Forex historiques. Assurez-vous d'avoir les donn√©es dans le chemin sp√©cifi√© dans `config.py`
2. **Temps d'entra√Ænement** : L'entra√Ænement peut prendre plusieurs heures selon la taille des donn√©es
3. **Risques** : Ce bot est √† des fins √©ducatives/recherche. Ne l'utilisez pas avec de l'argent r√©el sans tests approfondis
4. **Commissions** : Les r√©sultats sont meilleurs sans commissions. En conditions r√©elles avec commissions, les performances peuvent √™tre tr√®s diff√©rentes

## üÜò D√©pannage

### Probl√®me : Module non trouv√©
```bash
# Assurez-vous d'√™tre dans le r√©pertoire du projet
cd /media/gryphen/Disque\ local/SERIE/LINUX/RL-trading-main

# V√©rifiez que les chemins Python incluent le projet
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

### Probl√®me : Donn√©es manquantes
V√©rifiez que les donn√©es Forex sont pr√©sentes dans le chemin sp√©cifi√© dans `config.py`

### Probl√®me : Erreur avec gym
```bash
pip install setuptools==65.5.0 pip==21 wheel==0.38.0
pip install gym==0.21.0
```

## üìö Ressources suppl√©mentaires

- Documentation Stable-Baselines3 : https://stable-baselines3.readthedocs.io/
- Documentation Gym : https://gymnasium.farama.org/
- Paper de r√©f√©rence : "Financial Trading as a Game: A Deep Reinforcement Learning Approach"

---

**Bon trading ! üöÄ**

